{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IuyjxeJH5l5h"
   },
   "source": [
    "Importing the Libraries.\n",
    "\n",
    "PANDAS :Pandas provides us with some powerful objects like DataFrames and Series which are very useful for working with and analyzing data\n",
    "\n",
    "NUMPY :numpy library which provides objects for multi-dimensional arrays .\n",
    "\n",
    "TENSORFLOW :It is an open source artificial intelligence library, using data flow graphs to build models. It allows developers to create large-scale neural networks with many layers. TensorFlow is mainly used for: Classification, Perception, Understanding, Discovering, Prediction and Creation.\n",
    "\n",
    "KERAS :Keras is a neural network library . It provides only high-level APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jhxna0kq5jWw"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RYdrF35K65Qu"
   },
   "source": [
    "Read Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fqo_nyk-5jW_"
   },
   "outputs": [],
   "source": [
    "x_train =pd.read_csv(\"Train.csv\" )\n",
    "x_test =pd.read_csv(\"Test.csv\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0tB2LkTL7PJU"
   },
   "source": [
    "ILOC : iloc returns a Pandas Series when one row is selected, and a Pandas DataFrame when multiple rows are selected, or if any column in full is selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eANLMdUD5jXJ"
   },
   "outputs": [],
   "source": [
    "X_train =x_train.iloc[:,2:-1]\n",
    "y_train = x_train.iloc[:,-1]\n",
    "X_test = x_test.iloc[:,2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L8ivzJRX7XAC"
   },
   "source": [
    "SHAPE : shape is a tuple that gives you an indication of the number of dimensions in the array. So in your case, since the index value of Y. shape[0] is 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2chffwzX5jXT",
    "outputId": "25534c85-7c5c-401a-e571-7b0bbb3a81fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23856, 15)\n",
      "(15903, 15)\n",
      "(23856,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-6XLZMo97aXX"
   },
   "source": [
    "Checking for the missing value in dataset .\n",
    "ISNA() : Pandas dataframe. isna() function is used to detect missing values. It return a boolean same-sized object indicating if the values are NA. NA values, such as None or numpy. NaN, gets mapped to True values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vzQpLogS5jXh",
    "outputId": "b60e1a8b-91ba-4cfa-e4c6-c232abe7530c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "They are 1 columns with missing values : \n",
      "[('X_12', 182)] \n",
      "They are 1 columns with missing values : \n",
      "[('X_12', 127)] \n"
     ]
    }
   ],
   "source": [
    "missing =X_train.isna().sum(axis=0).sort_values(ascending=False)\n",
    "missing_value_columns = missing[missing>0]\n",
    "print('They are %s columns with missing values : \\n%s ' %(missing_value_columns.count() , [(index , value) for (index , value) in missing_value_columns.iteritems()]))\n",
    "\n",
    "missing =X_test.isna().sum(axis=0).sort_values(ascending=False)\n",
    "missing_value_columns = missing[missing>0]\n",
    "print('They are %s columns with missing values : \\n%s ' %(missing_value_columns.count() , [(index , value) for (index , value) in missing_value_columns.iteritems()]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IGZit58J7okw"
   },
   "source": [
    "Dealing with the missing value .\n",
    "FILLNA() : fillna fills the NaN values with a given number with which you want to substitute. It gives you an option to fill according to the index of rows of a pd. DataFrame or on the name of the columns in the form of a python dict ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m8SkzoXD5jXq"
   },
   "outputs": [],
   "source": [
    "def impute_value(X):\n",
    "    dataset =X\n",
    "    dataset['X_12'].fillna(dataset['X_12'].median() , inplace = True)\n",
    "    return dataset\n",
    "\n",
    "X_train = impute_value(X_train)\n",
    "X_test = impute_value(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bvnt0nPZ706M"
   },
   "source": [
    "Cross checking if any missing value is present ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wwjm7_tf5jXy",
    "outputId": "9039d8f1-15a9-4320-ad0b-e8ec26d4f7b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "They are 0 columns with missing values : \n",
      "[] \n",
      "They are 0 columns with missing values : \n",
      "[] \n"
     ]
    }
   ],
   "source": [
    "missing =X_train.isna().sum(axis=0).sort_values(ascending=False)\n",
    "missing_value_columns = missing[missing>0]\n",
    "print('They are %s columns with missing values : \\n%s ' %(missing_value_columns.count() , [(index , value) for (index , value) in missing_value_columns.iteritems()]))\n",
    "\n",
    "missing =X_test.isna().sum(axis=0).sort_values(ascending=False)\n",
    "missing_value_columns = missing[missing>0]\n",
    "print('They are %s columns with missing values : \\n%s ' %(missing_value_columns.count() , [(index , value) for (index , value) in missing_value_columns.iteritems()]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZuVGIULi8Mp_"
   },
   "source": [
    "Checking for the categorical value present in dataset ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UJNFqodc5jYG",
    "outputId": "406f4582-3ea9-4173-e03e-48d9d55e34ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the 0 object variables : \n",
      " []\n",
      "Here are the 0 object variables : \n",
      " []\n"
     ]
    }
   ],
   "source": [
    "object = list(X_train.select_dtypes(include=[np.object]))\n",
    "print('Here are the %s object variables : \\n %s' %(len(object) , object))\n",
    "\n",
    "object = list(X_test.select_dtypes(include=[np.object]))\n",
    "print('Here are the %s object variables : \\n %s' %(len(object) , object))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L-XSE0p_8V0a"
   },
   "source": [
    "Converting into numpy array .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "52TXvfaJ5jYO",
    "outputId": "a595bbec-1d67-4360-e066-4a3cace5dea1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23856, 15)\n",
      "(15903, 15)\n",
      "(23856,)\n"
     ]
    }
   ],
   "source": [
    "X_train =X_train.values\n",
    "# print(X_train.dtypes) :: to checkkk if the X_train is converted into numpy array or not [if error occur it is , if not it's not]\n",
    "X_test =X_test.values\n",
    "y_train=y_train.values\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-3ohmv838evA"
   },
   "source": [
    "Feature Scaling .\n",
    "Feature Scaling is a technique to standardize the independent features present in the data in a fixed range. It is performed during the data pre-processing to handle highly varying magnitudes or values or units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zciEuRwX5jYX"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3IYyy1lq8pyc"
   },
   "source": [
    "Sequential model : A Sequential model is appropriate for a plain stack of layers where each layer has exactly one input tensor and one output tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wUmFKXfc5jYx"
   },
   "outputs": [],
   "source": [
    "ann = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_ZKd4Gu-88Ti"
   },
   "source": [
    "1st Layer [hidden layer]\n",
    "with activation function : Rectified Linear Unit (ReLU) \n",
    "it is defined as y = max(0, x).\n",
    "and 6 Units ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Emof277o5jY_"
   },
   "outputs": [],
   "source": [
    "ann.add(tf.keras.layers.Dense(units=6 ,activation = 'relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NnODN2rU9VmV"
   },
   "source": [
    "2nd Layer [hidden layer] \n",
    "with activation function : Rectified Linear Unit (ReLU) \n",
    "it is defined as y = max(0, x). \n",
    "and 6 Units ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IWyVEzqr5jZI"
   },
   "outputs": [],
   "source": [
    "ann.add(tf.keras.layers.Dense(units=6 ,activation = 'relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U5ydVsyt9eFz"
   },
   "source": [
    "3rd Layer [output layer] \n",
    "with activation function :  sigmoid function\n",
    "we use sigmoid function is because it exists between (0 to 1). Therefore, it is especially used for models where we have to predict the probability as an output. Since probability of anything exists only between the range of 0 and 1, sigmoid is the right choice.\n",
    "and 1 Unit ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ITkebrE05jZR"
   },
   "outputs": [],
   "source": [
    "ann.add(tf.keras.layers.Dense(units=1 ,activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qkFKEaw292ue"
   },
   "source": [
    "Model Compile\n",
    "METRICS : A metric is a function that is used to judge the performance of your model. Metric functions are similar to loss functions, except that the results from evaluating a metric are not used when training the model.\n",
    "\n",
    "OPTIMIZER :Optimizers are algorithms or methods used to change the attributes of your neural network such as weights and learning rate in order to reduce the losses. Optimizers help to get results faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P86RlSkq5jZY"
   },
   "outputs": [],
   "source": [
    "ann.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d82ZWrdG-qR-"
   },
   "source": [
    "Fitting ;\n",
    "Fit function adjusts weights according to data values so that better accuracy can be achieved. After training, the model can be used for predictions ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fu_hiWa-5jZk",
    "outputId": "811990d5-5c70-493e-d72f-9dd77ab5d208"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "23856/23856 [==============================] - 1s 61us/sample - loss: 0.0480 - acc: 0.9812\n",
      "Epoch 2/100\n",
      "23856/23856 [==============================] - 2s 73us/sample - loss: 0.0481 - acc: 0.9813\n",
      "Epoch 3/100\n",
      "23856/23856 [==============================] - 2s 64us/sample - loss: 0.0481 - acc: 0.98141s - los\n",
      "Epoch 4/100\n",
      "23856/23856 [==============================] - 1s 61us/sample - loss: 0.0478 - acc: 0.9812\n",
      "Epoch 5/100\n",
      "23856/23856 [==============================] - 1s 48us/sample - loss: 0.0477 - acc: 0.9812\n",
      "Epoch 6/100\n",
      "23856/23856 [==============================] - 2s 68us/sample - loss: 0.0479 - acc: 0.98100s - loss: 0.0487 -\n",
      "Epoch 7/100\n",
      "23856/23856 [==============================] - 1s 53us/sample - loss: 0.0479 - acc: 0.9811\n",
      "Epoch 8/100\n",
      "23856/23856 [==============================] - 1s 50us/sample - loss: 0.0475 - acc: 0.9813\n",
      "Epoch 9/100\n",
      "23856/23856 [==============================] - 1s 46us/sample - loss: 0.0476 - acc: 0.9813\n",
      "Epoch 10/100\n",
      "23856/23856 [==============================] - 1s 53us/sample - loss: 0.0475 - acc: 0.9815\n",
      "Epoch 11/100\n",
      "23856/23856 [==============================] - 1s 62us/sample - loss: 0.0474 - acc: 0.9817\n",
      "Epoch 12/100\n",
      "23856/23856 [==============================] - 2s 65us/sample - loss: 0.0473 - acc: 0.9812\n",
      "Epoch 13/100\n",
      "23856/23856 [==============================] - 2s 64us/sample - loss: 0.0475 - acc: 0.9815\n",
      "Epoch 14/100\n",
      "23856/23856 [==============================] - 1s 56us/sample - loss: 0.0473 - acc: 0.9812\n",
      "Epoch 15/100\n",
      "23856/23856 [==============================] - 1s 44us/sample - loss: 0.0473 - acc: 0.9811\n",
      "Epoch 16/100\n",
      "23856/23856 [==============================] - 1s 42us/sample - loss: 0.0472 - acc: 0.9814\n",
      "Epoch 17/100\n",
      "23856/23856 [==============================] - 1s 49us/sample - loss: 0.0473 - acc: 0.9814\n",
      "Epoch 18/100\n",
      "23856/23856 [==============================] - 2s 70us/sample - loss: 0.0474 - acc: 0.9817\n",
      "Epoch 19/100\n",
      "23856/23856 [==============================] - 2s 78us/sample - loss: 0.0471 - acc: 0.9816\n",
      "Epoch 20/100\n",
      "23856/23856 [==============================] - 2s 72us/sample - loss: 0.0470 - acc: 0.98180s - loss: 0.0464 - acc:\n",
      "Epoch 21/100\n",
      "23856/23856 [==============================] - 1s 45us/sample - loss: 0.0470 - acc: 0.9816\n",
      "Epoch 22/100\n",
      "23856/23856 [==============================] - 2s 88us/sample - loss: 0.0468 - acc: 0.98170s - loss: 0.0479 - acc: 0\n",
      "Epoch 23/100\n",
      "23856/23856 [==============================] - 2s 79us/sample - loss: 0.0469 - acc: 0.9817\n",
      "Epoch 24/100\n",
      "23856/23856 [==============================] - 1s 55us/sample - loss: 0.0470 - acc: 0.9816\n",
      "Epoch 25/100\n",
      "23856/23856 [==============================] - 1s 50us/sample - loss: 0.0468 - acc: 0.9817\n",
      "Epoch 26/100\n",
      "23856/23856 [==============================] - 1s 61us/sample - loss: 0.0469 - acc: 0.98131s - l\n",
      "Epoch 27/100\n",
      "23856/23856 [==============================] - 1s 52us/sample - loss: 0.0471 - acc: 0.9817\n",
      "Epoch 28/100\n",
      "23856/23856 [==============================] - 1s 62us/sample - loss: 0.0467 - acc: 0.9816\n",
      "Epoch 29/100\n",
      "23856/23856 [==============================] - 1s 61us/sample - loss: 0.0468 - acc: 0.9817\n",
      "Epoch 30/100\n",
      "23856/23856 [==============================] - 2s 66us/sample - loss: 0.0468 - acc: 0.9817\n",
      "Epoch 31/100\n",
      "23856/23856 [==============================] - 1s 63us/sample - loss: 0.0465 - acc: 0.98191s - \n",
      "Epoch 32/100\n",
      "23856/23856 [==============================] - 1s 57us/sample - loss: 0.0466 - acc: 0.9824\n",
      "Epoch 33/100\n",
      "23856/23856 [==============================] - 1s 49us/sample - loss: 0.0464 - acc: 0.9819\n",
      "Epoch 34/100\n",
      "23856/23856 [==============================] - 1s 43us/sample - loss: 0.0467 - acc: 0.9820\n",
      "Epoch 35/100\n",
      "23856/23856 [==============================] - 1s 38us/sample - loss: 0.0462 - acc: 0.9820\n",
      "Epoch 36/100\n",
      "23856/23856 [==============================] - 1s 39us/sample - loss: 0.0466 - acc: 0.9818\n",
      "Epoch 37/100\n",
      "23856/23856 [==============================] - 1s 46us/sample - loss: 0.0467 - acc: 0.9822\n",
      "Epoch 38/100\n",
      "23856/23856 [==============================] - 1s 61us/sample - loss: 0.0465 - acc: 0.9822\n",
      "Epoch 39/100\n",
      "23856/23856 [==============================] - 1s 60us/sample - loss: 0.0465 - acc: 0.9818\n",
      "Epoch 40/100\n",
      "23856/23856 [==============================] - 1s 62us/sample - loss: 0.0462 - acc: 0.9816\n",
      "Epoch 41/100\n",
      "23856/23856 [==============================] - 2s 65us/sample - loss: 0.0463 - acc: 0.9817\n",
      "Epoch 42/100\n",
      "23856/23856 [==============================] - 1s 61us/sample - loss: 0.0465 - acc: 0.9821\n",
      "Epoch 43/100\n",
      "23856/23856 [==============================] - 1s 61us/sample - loss: 0.0461 - acc: 0.9815\n",
      "Epoch 44/100\n",
      "23856/23856 [==============================] - 1s 59us/sample - loss: 0.0462 - acc: 0.9821\n",
      "Epoch 45/100\n",
      "23856/23856 [==============================] - 1s 50us/sample - loss: 0.0463 - acc: 0.9825\n",
      "Epoch 46/100\n",
      "23856/23856 [==============================] - 1s 52us/sample - loss: 0.0463 - acc: 0.9815\n",
      "Epoch 47/100\n",
      "23856/23856 [==============================] - 1s 56us/sample - loss: 0.0463 - acc: 0.9817\n",
      "Epoch 48/100\n",
      "23856/23856 [==============================] - 2s 74us/sample - loss: 0.0461 - acc: 0.9817\n",
      "Epoch 49/100\n",
      "23856/23856 [==============================] - 2s 69us/sample - loss: 0.0462 - acc: 0.9823\n",
      "Epoch 50/100\n",
      "23856/23856 [==============================] - 2s 68us/sample - loss: 0.0460 - acc: 0.9818\n",
      "Epoch 51/100\n",
      "23856/23856 [==============================] - 2s 70us/sample - loss: 0.0462 - acc: 0.9820\n",
      "Epoch 52/100\n",
      "23856/23856 [==============================] - 1s 49us/sample - loss: 0.0459 - acc: 0.9823\n",
      "Epoch 53/100\n",
      "23856/23856 [==============================] - 1s 53us/sample - loss: 0.0460 - acc: 0.9818\n",
      "Epoch 54/100\n",
      "23856/23856 [==============================] - 3s 106us/sample - loss: 0.0458 - acc: 0.9815s - loss: 0.0445 - acc:\n",
      "Epoch 55/100\n",
      "23856/23856 [==============================] - ETA: 0s - loss: 0.0462 - acc: 0.9817- ETA: 0s - loss: 0.0463 - acc: 0.98 - 2s 82us/sample - loss: 0.0460 - acc: 0.9817\n",
      "Epoch 56/100\n",
      "23856/23856 [==============================] - 2s 63us/sample - loss: 0.0462 - acc: 0.9822\n",
      "Epoch 57/100\n",
      "23856/23856 [==============================] - 1s 63us/sample - loss: 0.0459 - acc: 0.9820\n",
      "Epoch 58/100\n",
      "23856/23856 [==============================] - 1s 62us/sample - loss: 0.0462 - acc: 0.9821\n",
      "Epoch 59/100\n",
      "23856/23856 [==============================] - 2s 67us/sample - loss: 0.0459 - acc: 0.9821\n",
      "Epoch 60/100\n",
      "23856/23856 [==============================] - 2s 65us/sample - loss: 0.0460 - acc: 0.9821\n",
      "Epoch 61/100\n",
      "23856/23856 [==============================] - 1s 57us/sample - loss: 0.0461 - acc: 0.98240s - loss: 0.0471 - acc: 0\n",
      "Epoch 62/100\n",
      "23856/23856 [==============================] - 1s 44us/sample - loss: 0.0460 - acc: 0.9821\n",
      "Epoch 63/100\n",
      "23856/23856 [==============================] - 1s 40us/sample - loss: 0.0459 - acc: 0.9825\n",
      "Epoch 64/100\n",
      "23856/23856 [==============================] - 1s 47us/sample - loss: 0.0458 - acc: 0.9823\n",
      "Epoch 65/100\n",
      "23856/23856 [==============================] - 1s 52us/sample - loss: 0.0456 - acc: 0.9821\n",
      "Epoch 66/100\n",
      "23856/23856 [==============================] - 1s 57us/sample - loss: 0.0458 - acc: 0.9825\n",
      "Epoch 67/100\n",
      "23856/23856 [==============================] - 1s 56us/sample - loss: 0.0458 - acc: 0.9826\n",
      "Epoch 68/100\n",
      "23856/23856 [==============================] - 1s 55us/sample - loss: 0.0459 - acc: 0.9823\n",
      "Epoch 69/100\n",
      "23856/23856 [==============================] - 1s 58us/sample - loss: 0.0461 - acc: 0.9822\n",
      "Epoch 70/100\n",
      "23856/23856 [==============================] - 1s 55us/sample - loss: 0.0460 - acc: 0.9822\n",
      "Epoch 71/100\n",
      "23856/23856 [==============================] - 1s 53us/sample - loss: 0.0459 - acc: 0.98241s - loss:\n",
      "Epoch 72/100\n",
      "23856/23856 [==============================] - 1s 48us/sample - loss: 0.0458 - acc: 0.9822\n",
      "Epoch 73/100\n",
      "23856/23856 [==============================] - 1s 48us/sample - loss: 0.0458 - acc: 0.9822\n",
      "Epoch 74/100\n",
      "23856/23856 [==============================] - 1s 55us/sample - loss: 0.0455 - acc: 0.9826\n",
      "Epoch 75/100\n",
      "23856/23856 [==============================] - 1s 59us/sample - loss: 0.0457 - acc: 0.98211s - lo\n",
      "Epoch 76/100\n",
      "23856/23856 [==============================] - 1s 50us/sample - loss: 0.0458 - acc: 0.9823\n",
      "Epoch 77/100\n",
      "23856/23856 [==============================] - 1s 47us/sample - loss: 0.0457 - acc: 0.9822\n",
      "Epoch 78/100\n",
      "23856/23856 [==============================] - 1s 45us/sample - loss: 0.0457 - acc: 0.9823\n",
      "Epoch 79/100\n",
      "23856/23856 [==============================] - 1s 60us/sample - loss: 0.0457 - acc: 0.9822\n",
      "Epoch 80/100\n",
      "23856/23856 [==============================] - 1s 53us/sample - loss: 0.0458 - acc: 0.9821\n",
      "Epoch 81/100\n",
      "23856/23856 [==============================] - 1s 41us/sample - loss: 0.0457 - acc: 0.9821\n",
      "Epoch 82/100\n",
      "23856/23856 [==============================] - 1s 37us/sample - loss: 0.0457 - acc: 0.9820\n",
      "Epoch 83/100\n",
      "23856/23856 [==============================] - 1s 50us/sample - loss: 0.0456 - acc: 0.9821\n",
      "Epoch 84/100\n",
      "23856/23856 [==============================] - 2s 74us/sample - loss: 0.0456 - acc: 0.9826\n",
      "Epoch 85/100\n",
      "23856/23856 [==============================] - 2s 75us/sample - loss: 0.0456 - acc: 0.9825\n",
      "Epoch 86/100\n",
      "23856/23856 [==============================] - 1s 57us/sample - loss: 0.0458 - acc: 0.9824\n",
      "Epoch 87/100\n",
      "23856/23856 [==============================] - 1s 45us/sample - loss: 0.0457 - acc: 0.9823\n",
      "Epoch 88/100\n",
      "23856/23856 [==============================] - 1s 54us/sample - loss: 0.0457 - acc: 0.98230s - loss\n",
      "Epoch 89/100\n",
      "23856/23856 [==============================] - 1s 59us/sample - loss: 0.0455 - acc: 0.9825\n",
      "Epoch 90/100\n",
      "23856/23856 [==============================] - 1s 60us/sample - loss: 0.0454 - acc: 0.9824\n",
      "Epoch 91/100\n",
      "23856/23856 [==============================] - 1s 56us/sample - loss: 0.0454 - acc: 0.9823\n",
      "Epoch 92/100\n",
      "23856/23856 [==============================] - 1s 53us/sample - loss: 0.0455 - acc: 0.9824\n",
      "Epoch 93/100\n",
      "23856/23856 [==============================] - 1s 53us/sample - loss: 0.0455 - acc: 0.98241s - lo\n",
      "Epoch 94/100\n",
      "23856/23856 [==============================] - 1s 46us/sample - loss: 0.0456 - acc: 0.9827\n",
      "Epoch 95/100\n",
      "23856/23856 [==============================] - 1s 42us/sample - loss: 0.0456 - acc: 0.9824\n",
      "Epoch 96/100\n",
      "23856/23856 [==============================] - 1s 41us/sample - loss: 0.0455 - acc: 0.9821\n",
      "Epoch 97/100\n",
      "23856/23856 [==============================] - 1s 40us/sample - loss: 0.0453 - acc: 0.9824\n",
      "Epoch 98/100\n",
      "23856/23856 [==============================] - 1s 40us/sample - loss: 0.0457 - acc: 0.9824\n",
      "Epoch 99/100\n",
      "23856/23856 [==============================] - 1s 40us/sample - loss: 0.0453 - acc: 0.9828\n",
      "Epoch 100/100\n",
      "23856/23856 [==============================] - 1s 40us/sample - loss: 0.0453 - acc: 0.9820\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2679399c588>"
      ]
     },
     "execution_count": 105,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann.fit(X_train, y_train, batch_size = 100, epochs = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eRlYlNkh-_fl"
   },
   "source": [
    "Prediction \n",
    "predict() : given a trained model, predict the label of a new set of data. This method accepts one argument, the new data X_new (e.g. model. predict(X_new) ), and returns the learned label for each object in the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "96xL57xc5jZs",
    "outputId": "c608ab3f-05d9-4c8b-ac85-37105aea715e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " ...\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = ann.predict(X_test)\n",
    "y_pred = (y_pred > 0.9)\n",
    "# print(y_pred.reshape(len(y_pred) , 1))\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DWgPRkTo_OwX"
   },
   "source": [
    "Converting boolean array :\n",
    "true = 1\n",
    "false = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YKjTR8Ak5jZz"
   },
   "outputs": [],
   "source": [
    "y_pred = y_pred*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "75eOL7nk5jZ8",
    "outputId": "00072f3e-bfff-47fc-ef44-b377a2ab1137"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [1],\n",
       "       ...,\n",
       "       [1],\n",
       "       [1],\n",
       "       [1]])"
      ]
     },
     "execution_count": 108,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kr3e5hyf5jaD",
    "outputId": "0fc363a5-41cb-4177-e287-f6cfb334c6b6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15903, 1)"
      ]
     },
     "execution_count": 109,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ViR5L1Rf_ehB"
   },
   "source": [
    "Reshaping the Y_pred \n",
    "The reshape() function is used to give a new shape to an array without changing its data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GrGAASXJ5jaL"
   },
   "outputs": [],
   "source": [
    "y_pred = y_pred.reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8r9_NWA7_jV9"
   },
   "source": [
    "Creating Submission File to submit my prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xB8hbw6m5jaV"
   },
   "outputs": [],
   "source": [
    "my_submission = pd.DataFrame({'INCIDENT_ID' : x_test.iloc[: ,0].values , 'MULTIPLE_OFFENSE' : y_pred})\n",
    "my_submission.to_csv('Prediction.csv' , index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nm-D4RgF5jae"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Final.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
